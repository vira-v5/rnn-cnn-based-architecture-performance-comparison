{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, Adagrad\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new_complaints3.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data splited into feature and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df['new']\n",
    "y_data = df['Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         transworld systems inc \\nis trying to collect ...\n",
       "1         Over the past 2 weeks I have been receiving ex...\n",
       "2         Pioneer has committed several federal violatio...\n",
       "3         Previously on XX XX XXXX XX XX XXXX and XX XX ...\n",
       "4         Hello This complaint is against the three cred...\n",
       "                                ...                        \n",
       "597869    1  Mailing Address is incorrect \\n2  Date of B...\n",
       "597870    I made a purchase of ##### on XXXX XXXX #### u...\n",
       "597871    On XXXX XXXX #### I contacted XXXX XXXX who is...\n",
       "597872    I can not get from chase who services my mortg...\n",
       "597873    cfbp i would Like to file a complaint on Exper...\n",
       "Name: new, Length: 597874, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame converted to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data.values\n",
    "y = y_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data splitted into train and test set, the train set is 80 percent of the data, and the test set is 20 percent. The data is stratified based on the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_value, X_val_value, y_train_value, y_val_value = train_test_split(X, y, \n",
    "                                                                          test_size=0.4, \n",
    "                                                                          random_state=666, \n",
    "                                                                          stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary was built based on the unique words from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(X_train_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159487"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty weight matrix for words in document vocabulary, +1 because the word index dictionary value starting from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size+1, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = KeyedVectors.load('vectors.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_vector = vectors['UNK']\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train_value)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val_value[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I previously tried to settle this matter with experian about unauthorized inquiries on my consumer report i sent them a letter explaining the problem trying to get experian to resolve the issue but they never did On XX XX #### i sent out another letter certified about the unauthorized inquiries on my consumer report and i still have not got a response I requested there method of verification about these inquires and i never received anything from experian i will send a copy of the certified letter that i sent to experian on XX XX ####'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_value[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences of words padded and limited to 200 sequences per instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=200, truncating='post', padding='post')\n",
    "X_val = pad_sequences(X_val, maxlen=200, truncating='post', padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label converted to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "y_train_label = enc.fit_transform(y_train_value)\n",
    "y_val_label = enc.transform(y_val_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Checking or savings account' 'Credit card or prepaid card'\n",
      " 'Credit or consumer reporting, credit repair services' 'Debt collection'\n",
      " 'Money transfer or service, virtual currency' 'Mortgage' 'Personal loan'\n",
      " 'Student loan' 'Vehicle loan or lease']\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([ 15516,  38824, 145461,  74649,   7690,  42251,  11619,  16220,\n",
      "         6494], dtype=int64))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7, 8]), array([10344, 25882, 96974, 49767,  5126, 28168,  7746, 10813,  4330],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(enc.classes_)\n",
    "print(np.unique(y_train_label, return_counts=True))\n",
    "print(np.unique(y_val_label, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (358724, 200)\n",
      "Shape of label tensor: (358724, 9)\n",
      "Shape of label tensor: (239150, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(np.asarray(y_train_label))\n",
    "y_val = to_categorical(np.asarray(y_val_label))\n",
    "print('Shape of data tensor:', X_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "print('Shape of label tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data further for validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=666, stratify=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/validation/test\n",
      "358724/119575/119575\n"
     ]
    }
   ],
   "source": [
    "print('training/validation/test')\n",
    "print(str(X_train.shape[0]) + '/' + str(X_val.shape[0]) + '/' + str(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dropout, Conv1D, MaxPool1D, Flatten\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = vocab_size + 1\n",
    "output_dim = 300\n",
    "input_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=5,\n",
    "                              restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "rnn.add(Embedding(input_dim=input_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            input_length=input_length,\n",
    "                            trainable=True))\n",
    "rnn.add(LSTM(128))\n",
    "rnn.add(Dropout(0.2))\n",
    "rnn.add(Dense(9,activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "rnn.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 300)          47846400  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 48,067,209\n",
      "Trainable params: 48,067,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7175/7175 [==============================] - 6991s 974ms/step - loss: 0.8228 - acc: 0.7225 - val_loss: 0.4704 - val_acc: 0.8429\n",
      "Epoch 2/50\n",
      "7175/7175 [==============================] - 7595s 1s/step - loss: 0.4286 - acc: 0.8562 - val_loss: 0.4379 - val_acc: 0.8508\n",
      "Epoch 3/50\n",
      "7175/7175 [==============================] - 6778s 945ms/step - loss: 0.3497 - acc: 0.8820 - val_loss: 0.4526 - val_acc: 0.8517\n",
      "Epoch 4/50\n",
      "7175/7175 [==============================] - 7476s 1s/step - loss: 0.2827 - acc: 0.9050 - val_loss: 0.4779 - val_acc: 0.8495\n",
      "Epoch 5/50\n",
      "7175/7175 [==============================] - 6816s 950ms/step - loss: 0.2263 - acc: 0.9246 - val_loss: 0.5223 - val_acc: 0.8450\n",
      "Epoch 6/50\n",
      "7175/7175 [==============================] - 7313s 1s/step - loss: 0.1832 - acc: 0.9388 - val_loss: 0.5685 - val_acc: 0.8425\n",
      "Epoch 7/50\n",
      "7175/7175 [==============================] - 6560s 914ms/step - loss: 0.1491 - acc: 0.9506 - val_loss: 0.6476 - val_acc: 0.8377\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history5 = rnn.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=1,  \n",
    "                    validation_data=(X_val, y_val),callbacks =[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping3 = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=5,\n",
    "                              restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length=200\n",
    "cnn2 = Sequential()\n",
    "cnn2.add(Embedding(input_dim=input_dim,\n",
    "                            output_dim=output_dim,\n",
    "                            input_length=input_length,\n",
    "                            trainable=True))\n",
    "cnn2.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn2.add(Dropout(0.2))\n",
    "cnn2.add(MaxPool1D(5))\n",
    "cnn2.add(Flatten())\n",
    "cnn2.add(Dense(9,activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "cnn2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 200, 300)          47846400  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 196, 128)          192128    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 196, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4992)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 44937     \n",
      "=================================================================\n",
      "Total params: 48,083,465\n",
      "Trainable params: 48,083,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7175/7175 [==============================] - 5462s 761ms/step - loss: 0.5773 - acc: 0.8157 - val_loss: 0.5215 - val_acc: 0.8358\n",
      "Epoch 2/50\n",
      "7175/7175 [==============================] - 5459s 761ms/step - loss: 0.4386 - acc: 0.8594 - val_loss: 0.5389 - val_acc: 0.8361\n",
      "Epoch 3/50\n",
      "7175/7175 [==============================] - 5462s 761ms/step - loss: 0.3460 - acc: 0.8884 - val_loss: 0.5716 - val_acc: 0.8336\n",
      "Epoch 4/50\n",
      "7175/7175 [==============================] - 5452s 760ms/step - loss: 0.2685 - acc: 0.9130 - val_loss: 0.6627 - val_acc: 0.8263\n",
      "Epoch 5/50\n",
      "7175/7175 [==============================] - 5441s 758ms/step - loss: 0.2165 - acc: 0.9306 - val_loss: 0.7841 - val_acc: 0.8256\n",
      "Epoch 6/50\n",
      "7175/7175 [==============================] - 5678s 791ms/step - loss: 0.1857 - acc: 0.9412 - val_loss: 0.8738 - val_acc: 0.8239\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history7 = cnn2.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=1,  \n",
    "                    validation_data=(X_val, y_val),callbacks =[earlystopping3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping4 = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=5,\n",
    "                              restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length=200\n",
    "rnn_w2v = Sequential()\n",
    "rnn_w2v.add(Embedding(input_dim=input_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=input_length,\n",
    "                      trainable=False))\n",
    "rnn_w2v.add(LSTM(128))\n",
    "rnn_w2v.add(Dropout(0.2))\n",
    "rnn_w2v.add(Dense(9,activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "rnn_w2v.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 300)          47846400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 48,067,209\n",
      "Trainable params: 220,809\n",
      "Non-trainable params: 47,846,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_w2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7175/7175 [==============================] - 2251s 314ms/step - loss: 0.9046 - acc: 0.6932 - val_loss: 0.5397 - val_acc: 0.8189\n",
      "Epoch 2/50\n",
      "7175/7175 [==============================] - 2133s 297ms/step - loss: 0.5215 - acc: 0.8248 - val_loss: 0.4964 - val_acc: 0.8298\n",
      "Epoch 3/50\n",
      "7175/7175 [==============================] - 2327s 324ms/step - loss: 0.4759 - acc: 0.8384 - val_loss: 0.4780 - val_acc: 0.8369\n",
      "Epoch 4/50\n",
      "7175/7175 [==============================] - 2350s 328ms/step - loss: 0.4448 - acc: 0.8485 - val_loss: 0.4709 - val_acc: 0.8398\n",
      "Epoch 5/50\n",
      "7175/7175 [==============================] - 2245s 313ms/step - loss: 0.4199 - acc: 0.8558 - val_loss: 0.4559 - val_acc: 0.8449\n",
      "Epoch 6/50\n",
      "7175/7175 [==============================] - 2137s 298ms/step - loss: 0.3979 - acc: 0.8638 - val_loss: 0.4521 - val_acc: 0.8468\n",
      "Epoch 7/50\n",
      "7175/7175 [==============================] - 2095s 292ms/step - loss: 0.3774 - acc: 0.8706 - val_loss: 0.4552 - val_acc: 0.8477\n",
      "Epoch 8/50\n",
      "7175/7175 [==============================] - 2600s 362ms/step - loss: 0.3588 - acc: 0.8770 - val_loss: 0.4561 - val_acc: 0.8485\n",
      "Epoch 9/50\n",
      "7175/7175 [==============================] - 2300s 321ms/step - loss: 0.3403 - acc: 0.8835 - val_loss: 0.4705 - val_acc: 0.8474\n",
      "Epoch 10/50\n",
      "7175/7175 [==============================] - 2125s 296ms/step - loss: 0.3228 - acc: 0.8902 - val_loss: 0.4755 - val_acc: 0.8481\n",
      "Epoch 11/50\n",
      "7175/7175 [==============================] - 2188s 305ms/step - loss: 0.3069 - acc: 0.8955 - val_loss: 0.4844 - val_acc: 0.8467\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history8 = rnn_w2v.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=1,  \n",
    "                    validation_data=(X_val, y_val),callbacks =[earlystopping4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping7 = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=5,\n",
    "                              restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length=200\n",
    "cnn_w2v2 = Sequential()\n",
    "cnn_w2v2.add(Embedding(input_dim=input_dim,\n",
    "                      output_dim=output_dim,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=input_length,\n",
    "                      trainable=False))\n",
    "cnn_w2v2.add(Conv1D(128, 5, activation='relu'))\n",
    "cnn_w2v2.add(Dropout(0.2))\n",
    "cnn_w2v2.add(MaxPool1D(5))\n",
    "cnn_w2v2.add(Flatten())\n",
    "cnn_w2v2.add(Dense(9,activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "cnn_w2v2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 300)          47846400  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 196, 128)          192128    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 196, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4992)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 44937     \n",
      "=================================================================\n",
      "Total params: 48,083,465\n",
      "Trainable params: 237,065\n",
      "Non-trainable params: 47,846,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_w2v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7175/7175 [==============================] - 813s 113ms/step - loss: 0.6489 - acc: 0.7913 - val_loss: 0.5891 - val_acc: 0.8110\n",
      "Epoch 2/50\n",
      "7175/7175 [==============================] - 820s 114ms/step - loss: 0.5555 - acc: 0.8201 - val_loss: 0.5641 - val_acc: 0.8195\n",
      "Epoch 3/50\n",
      "7175/7175 [==============================] - 817s 114ms/step - loss: 0.5191 - acc: 0.8312 - val_loss: 0.5576 - val_acc: 0.8244\n",
      "Epoch 4/50\n",
      "7175/7175 [==============================] - 850s 118ms/step - loss: 0.4908 - acc: 0.8387 - val_loss: 0.5653 - val_acc: 0.8226\n",
      "Epoch 5/50\n",
      "7175/7175 [==============================] - 839s 117ms/step - loss: 0.4672 - acc: 0.8466 - val_loss: 0.5695 - val_acc: 0.8214\n",
      "Epoch 6/50\n",
      "7175/7175 [==============================] - 810s 113ms/step - loss: 0.4462 - acc: 0.8523 - val_loss: 0.5724 - val_acc: 0.8253\n",
      "Epoch 7/50\n",
      "7175/7175 [==============================] - 814s 113ms/step - loss: 0.4276 - acc: 0.8582 - val_loss: 0.5870 - val_acc: 0.8185\n",
      "Epoch 8/50\n",
      "7175/7175 [==============================] - 821s 114ms/step - loss: 0.4099 - acc: 0.8633 - val_loss: 0.6001 - val_acc: 0.8223\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history11 = cnn_w2v2.fit(X_train, y_train, epochs=50, batch_size=batch_size, verbose=1,  \n",
    "                    validation_data=(X_val, y_val),callbacks =[earlystopping7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping6 = EarlyStopping(monitor=\"val_loss\",\n",
    "                              mode=\"min\", patience=5,\n",
    "                              restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(history, model, history_file, model_json, model_weight):\n",
    "    with open(history_file, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "    # serialize model to JSON\n",
    "    model_to_json = model.to_json()\n",
    "    with open(model_json, \"w\") as json_file:\n",
    "        json_file.write(model_to_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_weight)\n",
    "    print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hist(hist, json_name, csv_name):\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(hist.history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = json_name  \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # or save to csv: \n",
    "    hist_csv_file = csv_name \n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hist(history9, 'history9.json', 'history9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "save_model(history11, cnn_w2v2,'trainHistoryDict_model_4_cnn_w2v2',\n",
    "          'model_4_cnn_w2v2.json', 'model_4_cnn_w2v2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
